{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "Text preprocessing includes a series of steps used to clean and prepare raw text data for analysis or modeling. It typically includes tasks such as removing noise (like HTML tags, URLs, punctuation, emojis, and emoticons), converting text to lowercase, correcting spelling, replacing abbreviations with full forms, removing stopwords, and normalizing words through stemming or lemmatization. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python3 -m pip install ipykernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Employee Name</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saisab</td>\n",
       "      <td>Fantastic place to work if you're passionate a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Greta</td>\n",
       "      <td>The &lt;b&gt;editing suite&lt;/b&gt; in Studio B keeps cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stanley</td>\n",
       "      <td>Post-production deadlines are brutal and exhau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chlo√©</td>\n",
       "      <td>Loved working on the new sci-fi project, super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Martin</td>\n",
       "      <td>&lt;div&gt;The cast and crew coordination has been f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ava</td>\n",
       "      <td>HR is attentive and really understands creativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wes</td>\n",
       "      <td>Cafeteria food on long shoot days needs an upg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kathryn</td>\n",
       "      <td>Finishing VFX on time is tough sometimes. http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jordan</td>\n",
       "      <td>The studio has a great culture for innovation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sofia</td>\n",
       "      <td>The script review and approval process is stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Spike</td>\n",
       "      <td>The workspace is clean, with excellent lightin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Patty</td>\n",
       "      <td>We need more training sessions on new filming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Christopher</td>\n",
       "      <td>Management is open to feedback from even junio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Barry</td>\n",
       "      <td>Great perks like free movie passes and wellnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mira</td>\n",
       "      <td>The director of our last film was really suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>James</td>\n",
       "      <td>I love the diversity of projects we get to wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lana</td>\n",
       "      <td>The equipment and tech are industry-leading!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Taika</td>\n",
       "      <td>Found a broken link in our internal script dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Denis</td>\n",
       "      <td>I've seen improvements in our production plann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Zoe</td>\n",
       "      <td>Workload is balanced and scheduling is fair.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Employee Name                                             Review\n",
       "0         Saisab  Fantastic place to work if you're passionate a...\n",
       "1          Greta  The <b>editing suite</b> in Studio B keeps cra...\n",
       "2        Stanley  Post-production deadlines are brutal and exhau...\n",
       "3          Chlo√©  Loved working on the new sci-fi project, super...\n",
       "4         Martin  <div>The cast and crew coordination has been f...\n",
       "5            Ava  HR is attentive and really understands creativ...\n",
       "6            Wes  Cafeteria food on long shoot days needs an upg...\n",
       "7        Kathryn  Finishing VFX on time is tough sometimes. http...\n",
       "8         Jordan  The studio has a great culture for innovation ...\n",
       "9          Sofia  The script review and approval process is stre...\n",
       "10         Spike  The workspace is clean, with excellent lightin...\n",
       "11         Patty  We need more training sessions on new filming ...\n",
       "12   Christopher  Management is open to feedback from even junio...\n",
       "13         Barry  Great perks like free movie passes and wellnes...\n",
       "14          Mira  The director of our last film was really suppo...\n",
       "15         James  I love the diversity of projects we get to wor...\n",
       "16          Lana       The equipment and tech are industry-leading!\n",
       "17         Taika  Found a broken link in our internal script dat...\n",
       "18         Denis  I've seen improvements in our production plann...\n",
       "19           Zoe       Workload is balanced and scheduling is fair."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "        \"Employee Name\": [\n",
    "        \"Saisab\", \"Greta\", \"Stanley\", \"Chlo√©\", \"Martin\", \"Ava\", \"Wes\", \"Kathryn\", \"Jordan\", \"Sofia\",\n",
    "        \"Spike\", \"Patty\", \"Christopher\", \"Barry\", \"Mira\", \"James\", \"Lana\", \"Taika\", \"Denis\", \"Zoe\"\n",
    "    ],\n",
    "    \"Review\": [\n",
    "        \"Fantastic place to work if you're passionate about films! #FilmLife üé¨\",\n",
    "        \"The <b>editing suite</b> in Studio B keeps crashing, please fix it ASAP! ;_;\",\n",
    "        \"Post-production deadlines are brutal and exhausting. üò°\",\n",
    "        \"Loved working on the new sci-fi project, super creative team! #OnSet üí°\",\n",
    "        \"<div>The cast and crew coordination has been flawless.</div>\",\n",
    "        \"HR is attentive and really understands creatives. ü§ó\",\n",
    "        \"Cafeteria food on long shoot days needs an upgrade. üçï‚òï\",\n",
    "        \"Finishing VFX on time is tough sometimes. https://www.vfxtracker.com\",\n",
    "        \"The studio has a great culture for innovation and expression! üòÑ\",\n",
    "        \"The script review and approval process is streamlined. üìùüé•\",\n",
    "        \"The workspace is clean, with excellent lighting for editors. üëå\",\n",
    "        \"We need more training sessions on new filming tech. :[\",\n",
    "        \"Management is open to feedback from even junior staff.\",\n",
    "        \"Great perks like free movie passes and wellness programs! üéâüçø\",\n",
    "        \"The director of our last film was really supportive.\",\n",
    "        \"I love the diversity of projects we get to work on.\",\n",
    "        \"The equipment and tech are industry-leading!\",\n",
    "        \"Found a broken link in our internal script database: <a href='https://example.com'>fix it</a>\",\n",
    "        \"I've seen improvements in our production planning process.\",\n",
    "        \"Workload is balanced and scheduling is fair.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fantastic place to work if you're passionate a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The &lt;b&gt;editing suite&lt;/b&gt; in Studio B keeps cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Post-production deadlines are brutal and exhau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Loved working on the new sci-fi project, super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;div&gt;The cast and crew coordination has been f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HR is attentive and really understands creativ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cafeteria food on long shoot days needs an upg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finishing VFX on time is tough sometimes. http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The studio has a great culture for innovation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The script review and approval process is stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The workspace is clean, with excellent lightin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>We need more training sessions on new filming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Management is open to feedback from even junio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Great perks like free movie passes and wellnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The director of our last film was really suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I love the diversity of projects we get to wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The equipment and tech are industry-leading!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Found a broken link in our internal script dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I've seen improvements in our production plann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Workload is balanced and scheduling is fair.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review\n",
       "0   Fantastic place to work if you're passionate a...\n",
       "1   The <b>editing suite</b> in Studio B keeps cra...\n",
       "2   Post-production deadlines are brutal and exhau...\n",
       "3   Loved working on the new sci-fi project, super...\n",
       "4   <div>The cast and crew coordination has been f...\n",
       "5   HR is attentive and really understands creativ...\n",
       "6   Cafeteria food on long shoot days needs an upg...\n",
       "7   Finishing VFX on time is tough sometimes. http...\n",
       "8   The studio has a great culture for innovation ...\n",
       "9   The script review and approval process is stre...\n",
       "10  The workspace is clean, with excellent lightin...\n",
       "11  We need more training sessions on new filming ...\n",
       "12  Management is open to feedback from even junio...\n",
       "13  Great perks like free movie passes and wellnes...\n",
       "14  The director of our last film was really suppo...\n",
       "15  I love the diversity of projects we get to wor...\n",
       "16       The equipment and tech are industry-leading!\n",
       "17  Found a broken link in our internal script dat...\n",
       "18  I've seen improvements in our production plann...\n",
       "19       Workload is balanced and scheduling is fair."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df = df[['Review']]\n",
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REPLACE SHORT FORM WORDS WITH FULL FORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisab31\\AppData\\Local\\Temp\\ipykernel_16256\\976252349.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['Review'] = review_df['Review'].apply(correct_short_forms)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fantastic place to work if you're passionate a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The &lt;b&gt;editing suite&lt;/b&gt; in Studio B keeps cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Post-production deadlines are brutal and exhau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Loved working on the new sci-fi project, super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;div&gt;The cast and crew coordination has been f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Human Resource is attentive and really underst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Cafeteria food on long shoot days needs an upg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Finishing Visual Effects on time is tough some...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The studio has a great culture for innovation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The script review and approval process is stre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The workspace is clean, with excellent lightin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>We need more training sessions on new filming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Management is open to feedback from even junio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Great perks like free movie passes and wellnes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The director of our last film was really suppo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I love the diversity of projects we get to wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The equipment and tech are industry-leading!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Found a broken link in our internal script dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I've seen improvements in our production plann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Workload is balanced and scheduling is fair.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review\n",
       "0   Fantastic place to work if you're passionate a...\n",
       "1   The <b>editing suite</b> in Studio B keeps cra...\n",
       "2   Post-production deadlines are brutal and exhau...\n",
       "3   Loved working on the new sci-fi project, super...\n",
       "4   <div>The cast and crew coordination has been f...\n",
       "5   Human Resource is attentive and really underst...\n",
       "6   Cafeteria food on long shoot days needs an upg...\n",
       "7   Finishing Visual Effects on time is tough some...\n",
       "8   The studio has a great culture for innovation ...\n",
       "9   The script review and approval process is stre...\n",
       "10  The workspace is clean, with excellent lightin...\n",
       "11  We need more training sessions on new filming ...\n",
       "12  Management is open to feedback from even junio...\n",
       "13  Great perks like free movie passes and wellnes...\n",
       "14  The director of our last film was really suppo...\n",
       "15  I love the diversity of projects we get to wor...\n",
       "16       The equipment and tech are industry-leading!\n",
       "17  Found a broken link in our internal script dat...\n",
       "18  I've seen improvements in our production plann...\n",
       "19       Workload is balanced and scheduling is fair."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_form_dict = {\n",
    "    'HR': 'Human Resources',\n",
    "    'VFX': 'Visual Effects',\n",
    "    'EHR': 'Electronic Health Record',  # Can be removed if not relevant; originally from hospital context\n",
    "    'ASAP': 'as soon as possible',\n",
    "    'DoP': 'Director of Photography',\n",
    "    'AD': 'Assistant Director',\n",
    "    'CGI': 'Computer Generated Imagery',\n",
    "    'DP': 'Director of Photography',  # Alternative abbreviation\n",
    "    'SFX': 'Special Effects',\n",
    "    'PA': 'Production Assistant',\n",
    "    'VO': 'Voice Over'\n",
    "}\n",
    "\n",
    "\n",
    "def correct_short_forms(text):\n",
    " \n",
    "    words = text.split()\n",
    "    corrected_words = [full_form_dict.get(word, word) for word in words]\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "    \n",
    "    return corrected_text\n",
    "\n",
    "\n",
    "review_df['Review'] = review_df['Review'].apply(correct_short_forms)\n",
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOWERCASING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisab31\\AppData\\Local\\Temp\\ipykernel_16256\\3005746257.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['Review']=review_df['Review'].str.lower()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fantastic place to work if you're passionate a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the &lt;b&gt;editing suite&lt;/b&gt; in studio b keeps cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post-production deadlines are brutal and exhau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loved working on the new sci-fi project, super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;div&gt;the cast and crew coordination has been f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  fantastic place to work if you're passionate a...\n",
       "1  the <b>editing suite</b> in studio b keeps cra...\n",
       "2  post-production deadlines are brutal and exhau...\n",
       "3  loved working on the new sci-fi project, super...\n",
       "4  <div>the cast and crew coordination has been f..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Review']=review_df['Review'].str.lower()\n",
    "review_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE HTML TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile(r'<.*?>') \n",
    "    return pattern.sub('', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisab31\\AppData\\Local\\Temp\\ipykernel_16256\\2960546677.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['Review'] = review_df['Review'].apply(lambda text: remove_html_tags(text))\n"
     ]
    }
   ],
   "source": [
    "review_df['Review'] = review_df['Review'].apply(lambda text: remove_html_tags(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the editing suite in studio b keeps crashing, please fix it asap! ;_;'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisab31\\AppData\\Local\\Temp\\ipykernel_16256\\4020123959.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['Review'] = review_df['Review'].apply(remove_url)\n"
     ]
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "review_df['Review'] = review_df['Review'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finishing visual effects on time is tough sometimes. '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Review'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE PUNCTUATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisab31\\AppData\\Local\\Temp\\ipykernel_16256\\1861039593.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['Review'] = review_df['Review'].apply(remove_punctuation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fantastic place to work if youre passionate ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the editing suite in studio b keeps crashing p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>postproduction deadlines are brutal and exhaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loved working on the new scifi project super c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the cast and crew coordination has been flawless</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0  fantastic place to work if youre passionate ab...\n",
       "1  the editing suite in studio b keeps crashing p...\n",
       "2  postproduction deadlines are brutal and exhaus...\n",
       "3  loved working on the new scifi project super c...\n",
       "4   the cast and crew coordination has been flawless"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    pattern = re.compile(f\"[{re.escape(string.punctuation)}]\")\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "review_df['Review'] = review_df['Review'].apply(remove_punctuation)\n",
    "review_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SPELLING CORRECTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtextblob\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcorrect_spelling\u001b[39m(text):\n\u001b[32m      4\u001b[39m     textBLB = TextBlob(text)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spelling(text):\n",
    "    textBLB = TextBlob(text)\n",
    "    return textBLB.correct().string\n",
    "\n",
    "review_df['Review'] = review_df['Review'].apply(correct_spelling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the director of our last film was really supportive'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Review'][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saisab31\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saisab31\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVING STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/fm-pc-\n",
      "[nltk_data]     lt-275/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\saisab31\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saisab31\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\saisab31/nltk_data'\n    - 'c:\\\\Users\\\\saisab31\\\\Desktop\\\\saisab\\\\ir\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\saisab31\\\\Desktop\\\\saisab\\\\ir\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\saisab31\\\\Desktop\\\\saisab\\\\ir\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\saisab31\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      8\u001b[39m     filtered_sentences = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(filtered_words)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_sentences\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m review_df[\u001b[33m'\u001b[39m\u001b[33mReview\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mreview_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mReview\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremoving_stopwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m review_df.head(\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mremoving_stopwords\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mremoving_stopwords\u001b[39m(text):\n\u001b[32m      5\u001b[39m     stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     words = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     filtered_words = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word.lower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m      8\u001b[39m     filtered_sentences = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(filtered_words)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saisab31\\Desktop\\saisab\\ir\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\saisab31/nltk_data'\n    - 'c:\\\\Users\\\\saisab31\\\\Desktop\\\\saisab\\\\ir\\\\venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\saisab31\\\\Desktop\\\\saisab\\\\ir\\\\venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\saisab31\\\\Desktop\\\\saisab\\\\ir\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\saisab31\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def removing_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    filtered_sentences = ' '.join(filtered_words)\n",
    "    \n",
    "    return filtered_sentences\n",
    "\n",
    "\n",
    "review_df['Review'] = review_df['Review'].apply(removing_stopwords)\n",
    "review_df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE EMOJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saisab31\\AppData\\Local\\Temp\\ipykernel_16256\\496643868.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['Review'] = review_df['Review'].apply(remove_emojis)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # Emojis\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "                               u\"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "                               u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                               u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                               u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                               u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                               u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                               u\"\\U0001FB00-\\U0001FBFF\"  # Symbols for Legacy Computing\n",
    "                               u\"\\U0001F004-\\U0001F0CF\"  # Miscellaneous Symbols and Arrows\n",
    "                               u\"\\U0001F10D-\\U0001F10F\"  # Enclosed Alphanumeric Supplement\n",
    "                               u\"\\U0001F200-\\U0001F251\"  # Enclosed Ideographic Supplement\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "review_df['Review'] = review_df['Review'].apply(remove_emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafeteria food needs improvement '"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Review'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVE EMOTICONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTICONS = {\n",
    "    u\":‚Äë\\)\":\"Happy face or smiley\",\n",
    "    u\":\\)\":\"Happy face or smiley\",\n",
    "    u\":-\\]\":\"Happy face or smiley\",\n",
    "    u\":\\]\":\"Happy face or smiley\",\n",
    "    u\":-3\":\"Happy face smiley\",\n",
    "    u\":3\":\"Happy face smiley\",\n",
    "    u\":->\":\"Happy face smiley\",\n",
    "    u\":>\":\"Happy face smiley\",\n",
    "    u\"8-\\)\":\"Happy face smiley\",\n",
    "    u\":o\\)\":\"Happy face smiley\",\n",
    "    u\":-\\}\":\"Happy face smiley\",\n",
    "    u\":\\}\":\"Happy face smiley\",\n",
    "    u\":-\\)\":\"Happy face smiley\",\n",
    "    u\":c\\)\":\"Happy face smiley\",\n",
    "    u\":\\^\\)\":\"Happy face smiley\",\n",
    "    u\"=\\]\":\"Happy face smiley\",\n",
    "    u\"=\\)\":\"Happy face smiley\",\n",
    "    u\":‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"X‚ÄëD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n",
    "    u\":-\\)\\)\":\"Very happy\",\n",
    "    u\":‚Äë\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‚Äëc\":\"Frown, sad, andry or pouting\",\n",
    "    u\":c\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‚Äë<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":<\":\"Frown, sad, andry or pouting\",\n",
    "    u\":‚Äë\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\[\":\"Frown, sad, andry or pouting\",\n",
    "    u\":\\{\":\"Frown, sad, andry or pouting\",\n",
    "    u\":@\":\"Frown, sad, andry or pouting\",\n",
    "    u\">:\\(\":\"Frown, sad, andry or pouting\",\n",
    "    u\":'‚Äë\\(\":\"Crying\",\n",
    "    u\":'\\(\":\"Crying\",\n",
    "    u\":'‚Äë\\)\":\"Tears of happiness\",\n",
    "    u\":'\\)\":\"Tears of happiness\",\n",
    "    u\"D‚Äë':\":\"Horror\",\n",
    "    u\"D:<\":\"Disgust\",\n",
    "    u\"D:\":\"Sadness\",\n",
    "    u\"D8\":\"Great dismay\",\n",
    "    u\"D;\":\"Great dismay\",\n",
    "    u\"D=\":\"Great dismay\",\n",
    "    u\"DX\":\"Great dismay\",\n",
    "    u\":‚ÄëO\":\"Surprise\",\n",
    "    u\":O\":\"Surprise\",\n",
    "    u\":‚Äëo\":\"Surprise\",\n",
    "    u\":o\":\"Surprise\",\n",
    "    u\":-0\":\"Shock\",\n",
    "    u\"8‚Äë0\":\"Yawn\",\n",
    "    u\">:O\":\"Yawn\",\n",
    "    u\":-\\*\":\"Kiss\",\n",
    "    u\":\\*\":\"Kiss\",\n",
    "    u\":X\":\"Kiss\",\n",
    "    u\";‚Äë\\)\":\"Wink or smirk\",\n",
    "    u\";\\)\":\"Wink or smirk\",\n",
    "    u\"\\*-\\)\":\"Wink or smirk\",\n",
    "    u\"\\*\\)\":\"Wink or smirk\",\n",
    "    u\";‚Äë\\]\":\"Wink or smirk\",\n",
    "    u\";\\]\":\"Wink or smirk\",\n",
    "    u\";\\^\\)\":\"Wink or smirk\",\n",
    "    u\":‚Äë,\":\"Wink or smirk\",\n",
    "    u\";D\":\"Wink or smirk\",\n",
    "    u\":‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"X‚ÄëP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‚Äë√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":√û\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\":‚Äë/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n",
    "    u\":‚Äë\\|\":\"Straight face\",\n",
    "    u\":\\|\":\"Straight face\",\n",
    "    u\":$\":\"Embarrassed or blushing\",\n",
    "    u\":‚Äëx\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‚Äë#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":‚Äë&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n",
    "    u\"O:‚Äë\\)\":\"Angel, saint or innocent\",\n",
    "    u\"O:\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:‚Äë3\":\"Angel, saint or innocent\",\n",
    "    u\"0:3\":\"Angel, saint or innocent\",\n",
    "    u\"0:‚Äë\\)\":\"Angel, saint or innocent\",\n",
    "    u\"0:\\)\":\"Angel, saint or innocent\",\n",
    "    u\":‚Äëb\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n",
    "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n",
    "    u\">:‚Äë\\)\":\"Evil or devilish\",\n",
    "    u\">:\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:‚Äë\\)\":\"Evil or devilish\",\n",
    "    u\"\\}:\\)\":\"Evil or devilish\",\n",
    "    u\"3:‚Äë\\)\":\"Evil or devilish\",\n",
    "    u\"3:\\)\":\"Evil or devilish\",\n",
    "    u\">;\\)\":\"Evil or devilish\",\n",
    "    u\"\\|;‚Äë\\)\":\"Cool\",\n",
    "    u\"\\|‚ÄëO\":\"Bored\",\n",
    "    u\":‚ÄëJ\":\"Tongue-in-cheek\",\n",
    "    u\"#‚Äë\\)\":\"Party all night\",\n",
    "    u\"%‚Äë\\)\":\"Drunk or confused\",\n",
    "    u\"%\\)\":\"Drunk or confused\",\n",
    "    u\":-###..\":\"Being sick\",\n",
    "    u\":###..\":\"Being sick\",\n",
    "    u\"<:‚Äë\\|\":\"Dump\",\n",
    "    u\"\\(>_<\\)\":\"Troubled\",\n",
    "    u\"\\(>_<\\)>\":\"Troubled\",\n",
    "    u\"\\(';'\\)\":\"Baby\",\n",
    "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(~_~;\\) \\(„Éª\\.„Éª;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n",
    "    u\"\\(-_-\\)zzz\":\"Sleeping\",\n",
    "    u\"\\(\\^_-\\)\":\"Wink\",\n",
    "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n",
    "    u\"\\(\\+o\\+\\)\":\"Confused\",\n",
    "    u\"\\(o\\|o\\)\":\"Ultraman\",\n",
    "    u\"\\^_\\^\":\"Joyful\",\n",
    "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\n",
    "    u\"\\(\\^O\\^\\)Ôºè\":\"Joyful\",\n",
    "    u\"\\(\\^o\\^\\)Ôºè\":\"Joyful\",\n",
    "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n",
    "    u\"\\('_'\\)\":\"Sad or Crying\",\n",
    "    u\"\\(/_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;_;\":\"Sad of Crying\",\n",
    "    u\"\\(;_:\\)\":\"Sad or Crying\",\n",
    "    u\"\\(;O;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(:_;\\)\":\"Sad or Crying\",\n",
    "    u\"\\(ToT\\)\":\"Sad or Crying\",\n",
    "    u\";_;\":\"Sad or Crying\",\n",
    "    u\";-;\":\"Sad or Crying\",\n",
    "    u\";n;\":\"Sad or Crying\",\n",
    "    u\";;\":\"Sad or Crying\",\n",
    "    u\"Q\\.Q\":\"Sad or Crying\",\n",
    "    u\"T\\.T\":\"Sad or Crying\",\n",
    "    u\"QQ\":\"Sad or Crying\",\n",
    "    u\"Q_Q\":\"Sad or Crying\",\n",
    "    u\"\\(-\\.-\\)\":\"Shame\",\n",
    "    u\"\\(-_-\\)\":\"Shame\",\n",
    "    u\"\\(‰∏Ä‰∏Ä\\)\":\"Shame\",\n",
    "    u\"\\(Ôºõ‰∏Ä_‰∏Ä\\)\":\"Shame\",\n",
    "    u\"\\(=_=\\)\":\"Tired\",\n",
    "    u\"\\(=\\^\\¬∑\\^=\\)\":\"cat\",\n",
    "    u\"\\(=\\^\\¬∑\\¬∑\\^=\\)\":\"cat\",\n",
    "    u\"=_\\^=\t\":\"cat\",\n",
    "    u\"\\(\\.\\.\\)\":\"Looking down\",\n",
    "    u\"\\(\\._\\.\\)\":\"Looking down\",\n",
    "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n",
    "    u\"\\(\\„Éª\\„Éª?\":\"Confusion\",\n",
    "    u\"\\(?_?\\)\":\"Confusion\",\n",
    "    u\">\\^_\\^<\":\"Normal Laugh\",\n",
    "    u\"<\\^!\\^>\":\"Normal Laugh\",\n",
    "    u\"\\^/\\^\":\"Normal Laugh\",\n",
    "    u\"\\Ôºà\\*\\^_\\^\\*Ôºâ\" :\"Normal Laugh\",\n",
    "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n",
    "    u\"\\(\\^‚Äî\\^\\Ôºâ\":\"Normal Laugh\",\n",
    "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n",
    "    u\"\\Ôºà\\^‚Äî\\^\\Ôºâ\":\"Waving\",\n",
    "    u\"\\(;_;\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\n",
    "    u\"\\(-_-\\)/~~~ \\($\\¬∑\\¬∑\\)/~~~\":\"Waving\",\n",
    "    u\"\\(T_T\\)/~~~\":\"Waving\",\n",
    "    u\"\\(ToT\\)/~~~\":\"Waving\",\n",
    "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n",
    "    u\"\\(\\*_\\*\\)\":\"Amazed\",\n",
    "    u\"\\(\\*_\\*;\":\"Amazed\",\n",
    "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n",
    "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n",
    "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n",
    "    u'\\(-\"-\\)':\"Worried\",\n",
    "    u\"\\(„Éº„Éº;\\)\":\"Worried\",\n",
    "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n",
    "    u\"\\(\\ÔºæÔΩñ\\Ôºæ\\)\":\"Happy\",\n",
    "    u\"\\(\\ÔºæÔΩï\\Ôºæ\\)\":\"Happy\",\n",
    "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^O\\^\\)\":\"Happy\",\n",
    "    u\"\\(\\^o\\^\\)\":\"Happy\",\n",
    "    u\"\\)\\^o\\^\\(\":\"Happy\",\n",
    "    u\":O o_O\":\"Surprised\",\n",
    "    u\"o_0\":\"Surprised\",\n",
    "    u\"o\\.O\":\"Surpised\",\n",
    "    u\"\\(o\\.o\\)\":\"Surprised\",\n",
    "    u\"oO\":\"Surprised\",\n",
    "    u\"\\(\\*Ôø£mÔø£\\)\":\"Dissatisfied\",\n",
    "    u\"\\(‚ÄòA`\\)\":\"Snubbed or Deflated\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8046/2114150310.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  review_df['Review'] = review_df['Review'].apply(remove_emoticons)\n"
     ]
    }
   ],
   "source": [
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "review_df['Review'] = review_df['Review'].apply(remove_emoticons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cafeteria food needs improvement '"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Review'][6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TOKENIZATION - There are many ways to implement tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_text = \"AI is getting smarter and can now make new things like art and stories.\"\n",
    "normal_para = \"AI is getting smarter and can now make new things like art and stories. This changes how people create and design.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the split function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI',\n",
       " 'is',\n",
       " 'getting',\n",
       " 'smarter',\n",
       " 'and',\n",
       " 'can',\n",
       " 'now',\n",
       " 'make',\n",
       " 'new',\n",
       " 'things',\n",
       " 'like',\n",
       " 'art',\n",
       " 'and',\n",
       " 'stories.']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# work tokenization\n",
    "tokenize1 = normal_text.split()\n",
    "tokenize1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI is getting smarter and can now make new things like art and stories',\n",
       " ' This changes how people create and design',\n",
       " '']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "tokenize2 = normal_para.split(\".\")\n",
    "tokenize2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI',\n",
       " 'is',\n",
       " 'getting',\n",
       " 'smarter',\n",
       " 'and',\n",
       " 'can',\n",
       " 'now',\n",
       " 'make',\n",
       " 'new',\n",
       " 'things',\n",
       " 'like',\n",
       " 'art',\n",
       " 'and',\n",
       " 'stories']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "tokenize3 = re.findall(\"[\\w']+\",normal_text)\n",
    "tokenize3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI',\n",
       " 'is',\n",
       " 'getting',\n",
       " 'smarter',\n",
       " 'and',\n",
       " 'can',\n",
       " 'now',\n",
       " 'make',\n",
       " 'new',\n",
       " 'things',\n",
       " 'like',\n",
       " 'art',\n",
       " 'and',\n",
       " 'stories',\n",
       " '.']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(normal_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI is getting smarter and can now make new things like art and stories.',\n",
       " 'This changes how people create and design.']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(normal_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "tokenize4 = nlp(normal_text)\n",
    "tokenize4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokenize4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    tokenize_value = nlp(text)\n",
    "    return tokenize_value\n",
    "\n",
    "review_df['Review'] = review_df['Review'].apply(spacy_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great company work techlife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>found table tennis board brokencan please fix sap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worklife balance terrible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0                        great company work techlife\n",
       "1  found table tennis board brokencan please fix sap\n",
       "2                         worklife balance terrible "
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEMMING\n",
    "\n",
    "Stemming is the process of reducint inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we again pass the tokenize value over stemming and then with the list comprehension  created the list of word and join then to show again in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def apply_stemming(text):\n",
    "    \n",
    "    tokenize_value = nlp(text)\n",
    "    \n",
    "    stemmed_words =  [stemmer.stem(token.text) for token in tokenize_value]\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    return stemmed_text\n",
    "\n",
    "review_df['Review'] = review_df['Review'].apply(apply_stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great company work techlife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>found table tennis board brokencan please fix sap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worklife balance terrible</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review\n",
       "0                        great company work techlife\n",
       "1  found table tennis board brokencan please fix sap\n",
       "2                         worklife balance terrible "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization, unlike Stemming , Reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word in scalled Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citatio nform of s set of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the final outcome after all these text-pre-processing is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>great company work techlife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>found table tennis board brokencan please fix sap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worklife balance terrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love new project amazing innovation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>team suppurative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>human resource department responsive helpful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cafeteria food needs improvement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>meetings deadline challenging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>company culture fantastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>company software dvelpmnt process quite efficient productive Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>office environment clean organized</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wish training opportunities ‚Äë</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>management unapproachable open feedback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>great benefits peaks employees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>project manager understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>love diversity team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>information technology infrastructure topnotch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>found broken link company websitehere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>give seen improvement software quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>workload manageable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            Review\n",
       "0                                      great company work techlife\n",
       "1                found table tennis board brokencan please fix sap\n",
       "2                                       worklife balance terrible \n",
       "3                             love new project amazing innovation \n",
       "4                                                 team suppurative\n",
       "5                    human resource department responsive helpful \n",
       "6                                cafeteria food needs improvement \n",
       "7                                    meetings deadline challenging\n",
       "8                                       company culture fantastic \n",
       "9   company software dvelpmnt process quite efficient productive Ô∏è\n",
       "10                             office environment clean organized \n",
       "11                                   wish training opportunities ‚Äë\n",
       "12                         management unapproachable open feedback\n",
       "13                                 great benefits peaks employees \n",
       "14                                   project manager understanding\n",
       "15                                             love diversity team\n",
       "16                  information technology infrastructure topnotch\n",
       "17                           found broken link company websitehere\n",
       "18                          give seen improvement software quality\n",
       "19                                             workload manageable"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
